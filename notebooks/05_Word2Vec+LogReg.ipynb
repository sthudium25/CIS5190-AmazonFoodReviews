{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42a2bb9-d00b-47be-a8ba-52195a734544",
   "metadata": {},
   "source": [
    "# Word2Vec & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c23843-8155-45b6-8b32-23a34e035f43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: imblearn in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (4.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim) (1.23.5)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: keras in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.12.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (0.4.8)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (2.12.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (1.54.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (65.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (4.22.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (4.13.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install wordcloud\n",
    "!pip install imblearn\n",
    "!pip install gensim\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b995ee-0462-477f-86e5-fc6610529e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "2023-04-26 20:45:40.460273: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-26 20:45:40.760650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-26 20:45:42.400640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Imports required libraries\n",
    "\n",
    "# for data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, datetime\n",
    "import string\n",
    "import multiprocessing\n",
    "\n",
    "# for NLP / sentiment analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# for visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# for model building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Keras:\n",
    "import keras\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "# from keras-preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# for local helper funtions\n",
    "import helper_module\n",
    "\n",
    "# for exporting cleaned data\n",
    "import os\n",
    "from os.path import join\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809408ad-72d6-4596-8c59-b744b0de0a5f",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "Data cleaned using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e5e13b-0a71-4c0a-b99f-bdf5c7e06317",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394052"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = pd.read_csv(\"Reviews_cleanText_noSW_sageMakerLocal.csv\")\n",
    "len(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fab45-4ec8-4ced-9816-e2ea101630df",
   "metadata": {},
   "source": [
    "Convert review text into a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5d7d1-046b-487f-85e7-c926e635cbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "  word_split = review.split()\n",
    "  words = [w for w in word_split]   \n",
    "  return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b931c406-0bda-4bce-bb6c-6de3fe15cb94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews_df['cleaned_text_list'] = reviews_df['cleaned_text'].apply(review_to_words)\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262a47c-1b4a-4d2f-a5c5-b00f4ce85986",
   "metadata": {},
   "source": [
    "Split train & test data, also up-sampling minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98750e6c-7a0a-4b4e-a33c-bfb3f73f52e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_df[\"cleaned_text_list\"], \n",
    "                                                    reviews_df[\"Score_class\"], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res_train, y_res_train = ros.fit_resample(X_train.array.reshape(-1, 1), y_train)\n",
    "\n",
    "X_res_train = pd.Series(X_res_train[:,0])\n",
    "\n",
    "print(\"%d items in training data, %d in test data\" % (len(X_res_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62397ca-8eff-4d13-8d4e-a080c3f4c90a",
   "metadata": {},
   "source": [
    "# Model Building: Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024a71a-07f6-4051-8d9e-4c837d226460",
   "metadata": {},
   "source": [
    "Observe the number of cores available for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a2106-2c99-4ac7-830a-05be2d6782a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "core_count = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "print(f'{core_count} cores are available for use')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa5611-1826-42b8-8bf8-4419c4f500aa",
   "metadata": {},
   "source": [
    "**The code below for \"model\" was manually adjusted for every run of the hyper-parameter tuning:**\n",
    "\n",
    "min_count: [10, 20, 50]\n",
    "\n",
    "window: [2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e0fed-bb8e-4c44-ae04-db6ad2afb633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize word to vec model \n",
    "model = Word2Vec(min_count=10,\n",
    "                 window=4,\n",
    "                 vector_size=300,\n",
    "                 sample=6e-5, \n",
    "                 workers= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9956c9-45ff-4aa4-aa1c-1d389444cf88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# build the vocabulary table \n",
    "model.build_vocab(X_res_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b376b-53bf-4587-bbc2-3c17d918ccd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train the model using X_res_train\n",
    "model.train(X_res_train, \n",
    "            total_examples = model.corpus_count, \n",
    "            epochs=10, \n",
    "            report_delay=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9801a8d-1fa1-4941-b162-7fbaa5b0d4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving trained KeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "model.wv.save('word2vec_min10_win4.kv')\n",
    "# reloaded_word_vectors = KeyedVectors.load('word2vec_50.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e513206-936e-40ae-8517-3dfc44ae1e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "#https://www.kaggle.com/code/arunava21/word2vec-and-random-forest-classification\n",
    "\n",
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    \n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.key_to_index)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model.wv[word])\n",
    "    \n",
    "    if nwords == 0: #when it comes to summary, words used might not be in the model trained\n",
    "        nwords = 1\n",
    "\n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all reviews\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for review in reviews:\n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return review_feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050bc3a-5a2c-4efc-9873-57265b3a54d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate average feature vectors for training and test sets\n",
    "clean_train_reviews = []\n",
    "for review in X_res_train:\n",
    "    clean_train_reviews.append(review)\n",
    "trainDataVecs = get_avg_feature_vecs(clean_train_reviews, model, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21911262-61f6-43e1-964f-ef77d3273d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clean_test_reviews = []\n",
    "for review in X_test:\n",
    "    clean_test_reviews.append(review)\n",
    "testDataVecs = get_avg_feature_vecs(clean_test_reviews, model, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d30562-cae0-4435-9b5b-6f2c5364f0e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainDataVecs_df = pd.DataFrame(trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c80037-5520-4d71-80a4-3bc04e3ae3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testDataVecs_df = pd.DataFrame(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77e5a3-13f5-4dab-9809-c29952752da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testDataVecs_df.to_csv(\"testDataVecs.csv\", index = False)\n",
    "# trainDataVecs_df.to_csv(\"trainDataVecs_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4260c2-3651-4b03-9160-fa029ca1f75c",
   "metadata": {},
   "source": [
    "# Model Building: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ae5c6-c562-4199-b60a-f6fad9426f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(max_iter = 1000)\n",
    "logreg = logreg.fit(trainDataVecs, y_res_train)\n",
    "prediction = dict()\n",
    "prediction['Logistic'] = logreg.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239b6ae-a329-4460-880c-bc64f40849bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auc, acc, f1, recall, cm = helper_module.model_eval(y_test, prediction[\"Logistic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa67c73-7007-4549-8d1a-1e6e41dd6683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model result\n",
    "name = \"W2V_LogReg_minWord10_win4\"\n",
    "helper_module.add_model_scores_to_results(file_path = 'Model_results.csv', \n",
    "                            model_name = name, \n",
    "                            datashift = 'test', \n",
    "                            with_sw = 0,\n",
    "                            ROC_AUC = auc, accuracy = acc, \n",
    "                            f1 = f1, recall=recall, cm = cm, first_entry=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed2804-f463-46c1-b852-1ef91954b1f6",
   "metadata": {},
   "source": [
    "# Data Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add842d2-c879-40bf-8f60-bc8ba6318f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summary Performance\n",
    "X_train_sum, X_test_sum, y_train_sum, y_test_sum = train_test_split(reviews_df[\"cleaned_summary\"], \n",
    "                                                    reviews_df[\"Score_class\"], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "clean_test_summary = []\n",
    "for review in X_test_sum:\n",
    "    clean_test_summary.append(review)\n",
    "    \n",
    "testSumVecs = get_avg_feature_vecs(clean_test_summary, model, 300)\n",
    "\n",
    "# predict on summary\n",
    "prediction = dict()\n",
    "prediction['Logistic'] = logreg.predict(testSumVecs)\n",
    "\n",
    "# get prediction scores\n",
    "auc, acc, f1, recall, cm = helper_module.model_eval(y_test, prediction[\"Logistic\"])\n",
    "\n",
    "# save result\n",
    "name = \"W2V_LogReg_minWord10_win4\"\n",
    "helper_module.add_model_scores_to_results(file_path = 'Model_results.csv', \n",
    "                            model_name = name, \n",
    "                            datashift = 'summary', \n",
    "                            with_sw = 0,\n",
    "                            ROC_AUC = auc, accuracy = acc, \n",
    "                            f1 = f1, recall=recall, cm = cm, first_entry=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db81bf-9e42-4520-b96f-eb3b7212a768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dropout performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_df[\"cleaned_text\"], \n",
    "                                                    reviews_df[\"Score_class\"], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "for i in [0.1, 0.25, 0.5]:\n",
    "    X_test_dropout = X_test.apply(helper_module.random_dropout,\n",
    "                                  p=i, random_state=42)\n",
    "    X_test_dropout_list = X_test_dropout.apply(review_to_words)\n",
    "    clean_test_review = []\n",
    "    for review in X_test_dropout_list:\n",
    "        clean_test_review.append(review)\n",
    "    testDataVecs = get_avg_feature_vecs(clean_test_review, model, 300)\n",
    "    \n",
    "    prediction = dict()\n",
    "    prediction['Logistic'] = logreg.predict(testDataVecs)\n",
    "    \n",
    "    auc, acc, f1, recall, cm = helper_module.model_eval(y_test, prediction[\"Logistic\"])\n",
    "    name = f'W2V_LogReg_minWord10_win+Dropout{i}'\n",
    "    helper_module.add_model_scores_to_results(file_path = 'Model_results.csv', \n",
    "                                model_name = name,\n",
    "                                datashift = f'dropout_{i}', with_sw = 0,\n",
    "                                ROC_AUC = auc, accuracy = acc, \n",
    "                                f1 = f1, recall=recall, cm = cm, first_entry=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d9cac7-71e5-4f9c-ab63-8530c72700be",
   "metadata": {},
   "source": [
    "# Build Embedding for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cf9e4-2984-4265-b1ac-7f1ff4266cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_doc_len=1951  # max lenght of a review\n",
    "vocab_size = len(tok.word_index) + 1  # total no of words\n",
    "embed_dim = 300 # embedding dimension as choosen in word2vec constructor\n",
    "print(f'Total number of words tokenized: {vocab_size}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f5839-9264-4e52-8f43-7a8e61c067ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# total number of extracted words learned by the Word2Vec model.\n",
    "vocab = model.wv.index_to_key\n",
    "print(\"The total number of words learned by Word2Vec model is : \",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea76b6a-b221-460a-ab18-1a7aec75f453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_vec_dict={}\n",
    "for word in vocab:\n",
    "    word_vec_dict[word]=model.wv.get_vector(word)\n",
    "print(\"The no of key-value pairs : \",len(word_vec_dict)) # should come equal to vocab size\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991678d-277b-4721-b2ea-1ff11be74c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now creating the embedding matrix\n",
    "embed_matrix = np.zeros(shape=(vocab_size,embed_dim))\n",
    "for word,i in tok.word_index.items():\n",
    "    embed_vector = word_vec_dict.get(word)\n",
    "    if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "        embed_matrix[i]=embed_vector\n",
    "  # if word is not found then embed_vector corressponding to that vector will stay zero.\n",
    "print(\"The shape of embed_matrix : \",embed_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dbcc26-e181-4455-b026-0f9c2e501008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the embedding matrix into a file\n",
    "embed_matrix_df = pd.DataFrame(embed_matrix)\n",
    "embed_matrix_df.to_csv(\"embed_matrix.csv\", index = False)\n",
    "# embed_matrix = pd.read_csv(\"mbed_matrix.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423cb7b7-6121-497b-8ce9-c276d6207e92",
   "metadata": {},
   "source": [
    "# References:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c0da2-2346-434b-9071-6705bcf9038f",
   "metadata": {},
   "source": [
    "**Gensim Word2Vec Tutorial** \n",
    "Provides the basic guidelines for Word2Vec\n",
    "https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial \n",
    "\n",
    "**word2vec and random forest classification** \n",
    "Provides the basic guidelines for Word2Vec\n",
    "https://www.kaggle.com/code/arunava21/word2vec-and-random-forest-classification \n",
    "\n",
    "**Amazon Fine Food Reviews: Sentiment Analysis.**\n",
    "Provides the basic guidelines for LSTM. https://www.kaggle.com/code/chirag9073/amazon-fine-food-reviews-sentiment-analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a113c2-f600-47ce-8685-c15ed4e2082b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res_train, y_res_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "# X_res_train = pd.Series(X_res_train[:,0])\n",
    "\n",
    "# print(\"%d items in training data, %d in test data\" % (len(X_res_train), len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c28db1-b700-4069-940c-dbd76d24fe33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/chirag9073/amazon-fine-food-reviews-sentiment-analysis\n",
    "def build_model(embedding_matrix):\n",
    "    words = Input(shape=(None,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    " \n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = Dense(512, activation='relu')(hidden)\n",
    "    \n",
    "    result = Dense(5, activation='softmax')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0795f8-962d-4288-b319-6869bc5d6a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = build_model(embedding_matrix)\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5', \n",
    "    monitor='val_acc', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    callbacks=[checkpoint],\n",
    "    epochs=10,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8f5ae-c5e9-469e-b395-c111f08bdcaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce33d56-9ea2-425c-a125-c8a545cf07f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b84d507-c5d7-49d8-b92a-7ea9d6ea96a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492018 items in training data, 78811 in test data\n",
      "CPU times: user 171 ms, sys: 35.9 ms, total: 206 ms\n",
      "Wall time: 207 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_df[\"cleaned_text\"], \n",
    "                                                    reviews_df[\"Score_class\"], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res_train, y_res_train = ros.fit_resample(X_train.array.reshape(-1, 1), y_train)\n",
    "\n",
    "# X_res_train = pd.Series(X_res_train[:,0])\n",
    "\n",
    "print(\"%d items in training data, %d in test data\" % (len(X_res_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c291687-00e2-409c-baf9-757571a3de04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting flair\n",
      "  Downloading flair-0.12.2-py3-none-any.whl (373 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.1/373.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]>=4.18.0\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from flair) (9.0.0)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch!=1.8,>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from flair) (1.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from flair) (2.8.2)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytorch-revgrad\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from flair) (1.26.114)\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting huggingface-hub>=0.10.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from flair) (3.5.3)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from flair) (0.9.0)\n",
      "Collecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gensim>=3.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from flair) (4.3.1)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.1\n",
      "  Downloading transformer_smaller_training_vocab-0.2.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from flair) (2022.10.31)\n",
      "Collecting janome\n",
      "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting segtok>=1.5.7\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from flair) (1.2.2)\n",
      "Collecting conllu>=4.0\n",
      "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting gdown==4.4.0\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from flair) (4.63.2)\n",
      "Collecting wikipedia-api\n",
      "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
      "Collecting pptree\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gdown==4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: requests[socks] in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gdown==4.4.0->flair) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gdown==4.4.0->flair) (3.6.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gdown==4.4.0->flair) (1.16.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from bpemb>=0.3.2->flair) (1.23.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim>=3.8.0->flair) (5.2.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim>=3.8.0->flair) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.10.0->flair) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.10.0->flair) (2022.11.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.10.0->flair) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.10.0->flair) (21.3)\n",
      "Requirement already satisfied: py4j in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from hyperopt>=0.2.7->flair) (0.10.9.5)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from hyperopt>=0.2.7->flair) (0.18.2)\n",
      "Requirement already satisfied: cloudpickle in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from hyperopt>=0.2.7->flair) (2.2.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from hyperopt>=0.2.7->flair) (3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n",
      "Collecting datasets<3.0.0,>=2.0.0\n",
      "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<=3.20.2\n",
      "  Downloading protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: botocore<1.30.0,>=1.29.114 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3->flair) (1.29.114)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3->flair) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3->flair) (0.6.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.114->boto3->flair) (1.26.8)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.4.4)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.70.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests[socks]->gdown==4.4.0->flair) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests[socks]->gdown==4.4.0->flair) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests[socks]->gdown==4.4.0->flair) (2.1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.2.post1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (2022.7)\n",
      "Building wheels for collected packages: gdown, mpld3, sqlitedict, langdetect, pptree\n",
      "  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14759 sha256=1240b3d4a5e60503718a11cb8c75a6866febbeeeeb10adee51de074b8f13392e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/91/79/92/b7383aa7cb8c51320976217882671ae0ed8e5cddead7b90024\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116685 sha256=b1758ca15da7f5372481ae43d69e8e6d789e36e036e97d771b50e05b9a7dfd41\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/2b/29/df/6d418d98833fa10b93a4a62999fecc462e28e193a12d705460\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=a68acb1f546e724af508633aacb831e5ce5cb2d0fe54011df7ea8a6206bcdd10\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/23/96/9a/a7c729ccc393f6ba66e52e60e8485fbfa44a0be1c399433ec0\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=04465450f4131def7de788f645db528adf4bdb32921b463474ca0e27d715bd49\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/6a/67/f8/9cf1a8ff87e0b37f738769df49cc142a655489a6d27b68089f\n",
      "  Building wheel for pptree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=c8e88e5cc494e91b1254fffeb9161bc192e3008030843cc286b1e730a79c763f\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/88/68/f2/d4c9dcd99ee0e209aa1b1e2679f6fbff1100af289e665c3bf5\n",
      "Successfully built gdown mpld3 sqlitedict langdetect pptree\n",
      "Installing collected packages: tokenizers, sqlitedict, sentencepiece, pptree, mpld3, janome, xxhash, segtok, protobuf, lxml, langdetect, ftfy, deprecated, conllu, wikipedia-api, responses, pytorch-revgrad, hyperopt, huggingface-hub, transformers, gdown, bpemb, datasets, transformer-smaller-training-vocab, flair\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.22.3\n",
      "    Uninstalling protobuf-4.22.3:\n",
      "      Successfully uninstalled protobuf-4.22.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bpemb-0.3.4 conllu-4.5.2 datasets-2.11.0 deprecated-1.2.13 flair-0.12.2 ftfy-6.1.1 gdown-4.4.0 huggingface-hub-0.14.1 hyperopt-0.2.7 janome-0.4.2 langdetect-1.0.9 lxml-4.9.2 mpld3-0.3 pptree-3.1 protobuf-3.20.2 pytorch-revgrad-0.2.0 responses-0.18.0 segtok-1.5.11 sentencepiece-0.1.98 sqlitedict-2.1.0 tokenizers-0.13.3 transformer-smaller-training-vocab-0.2.3 transformers-4.28.1 wikipedia-api-0.5.8 xxhash-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e03d28d-62d1-476a-87ac-44707e1f566d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9114ceaf-1150-426c-8e78-75df5e2a99c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'en-sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18977/1797037696.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the NER tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# make a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/flair/nn/model.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, model_path)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classifier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/flair/nn/model.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, model_path)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;31m# if the model cannot be fetched, load as a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mload_torch_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# try to get model class from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/flair/file_utils.py\u001b[0m in \u001b[0;36mload_torch_state\u001b[0;34m(model_file)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# to load models on some Mac/Windows setups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# see https://github.com/zalandoresearch/flair/issues/351\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_big_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/flair/file_utils.py\u001b[0m in \u001b[0;36mload_big_file\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;31m# mmap seems to be much more memory efficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACCESS_READ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'en-sentiment'"
     ]
    }
   ],
   "source": [
    "# load the NER tagger\n",
    "tagger = Classifier.load('en-sentiment')\n",
    "\n",
    "def predict(x):\n",
    "    # make a sentence\n",
    "    sentence = Sentence(x) \n",
    "    pred = tagger.predict(sentence)\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1fd0e77-7cfa-48b8-baae-158bdf083012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18977/2568995118.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/flair/nn/model.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, model_path)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classifier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/flair/nn/model.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, model_path)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;31m# if the model cannot be fetched, load as a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mload_torch_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# try to get model class from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/flair/file_utils.py\u001b[0m in \u001b[0;36mload_torch_state\u001b[0;34m(model_file)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# to load models on some Mac/Windows setups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# see https://github.com/zalandoresearch/flair/issues/351\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_big_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/flair/file_utils.py\u001b[0m in \u001b[0;36mload_big_file\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;31m# mmap seems to be much more memory efficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACCESS_READ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sentiment'"
     ]
    }
   ],
   "source": [
    "tagger = Classifier.load('sentiment')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b884a3c2-f386-4142-998f-b973db75772c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18977/586184452.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# run NER over sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print the sentence with all annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tagger' is not defined"
     ]
    }
   ],
   "source": [
    "sentence = Sentence(\"the food is bad\")\n",
    "\n",
    "# run NER over sentence\n",
    "pred = tagger.predict(sentence)\n",
    "\n",
    "# print the sentence with all annotations\n",
    "print(sentence)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3460cd-002e-465c-8a09-36343beb3150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

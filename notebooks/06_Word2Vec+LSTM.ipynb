{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec & LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 00:59:46.528158: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-27 00:59:46.581802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-27 00:59:47.743591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import gensim\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import pad_sequences\n",
    "# from keras.preprocessing.sequence import pad_sequences #deprecated\n",
    "from keras.models import Sequential\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394052"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Reviews_cleanText_noSW_sageMakerLocal.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394052, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(394052, 2)\n"
     ]
    }
   ],
   "source": [
    "rating_df = pd.DataFrame(df, columns=['Score_class', 'cleaned_text'])\n",
    "print(rating_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     307472\n",
       "-1     86580\n",
       "Name: Score_class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df['Score_class'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   -1   1\n",
       "0   0   1\n",
       "1   1   0\n",
       "2   0   1\n",
       "3   1   0\n",
       "4   0   1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(rating_df['Score_class'])\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(rating_df['cleaned_text'], \n",
    "                                                    dummies, \n",
    "                                                    test_size=0.1, random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_matrix = pd.read_csv(\"embed_matrix.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following model was trained multiple times with 3 different SpatialDropout1D(x) values:**\n",
    "Hyper-Parameter Value Tested: [0.2, 0.3, 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix):\n",
    "    words = Input(shape=(None,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.4)(x) # changed for 3 model runs\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    " \n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = Dense(512, activation='relu')(hidden)\n",
    "    \n",
    "    result = Dense(2, activation='softmax')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy','AUC','Precision','Recall']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxlen=512\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = pad_sequences(x_train, maxlen=512)\n",
    "x_test = pad_sequences(x_test, maxlen=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.backend.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = build_model(embedding_matrix)\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5', \n",
    "    monitor='val_acc', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    callbacks=[checkpoint],\n",
    "    epochs=5,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['accuracy'], label = 'train_accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.xticks([0,1,2,3,4])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig('LSTM_v3_Accuracy.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training & validation auc values\n",
    "plt.plot(history.history['val_auc'], label = 'val_auc')\n",
    "plt.plot(history.history['auc'], label = 'train_auc')\n",
    "plt.title('Model AUC')\n",
    "plt.legend()\n",
    "plt.xticks([0,1,2,3,4])\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig('LSTM_v3_AUC.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['val_loss'], label = \"val_loss\")\n",
    "plt.plot(history.history['loss'], label = \"training_loss\")\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "plt.xticks([0,1,2,3,4])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig('LSTM_v3_Loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training & validation recall values\n",
    "plt.plot(history.history['val_recall'], label = 'val_recall')\n",
    "plt.plot(history.history['recall'], label = 'train_recall')\n",
    "plt.title('Model Recall')\n",
    "plt.legend()\n",
    "plt.xticks([0,1,2,3,4])\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig('LSTM_v3_Recall.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training & validation recall values\n",
    "plt.plot(history.history['val_precision'], label = 'val_precision')\n",
    "plt.plot(history.history['precision'], label = 'train_precision')\n",
    "plt.title('Model Precision')\n",
    "plt.legend()\n",
    "plt.xticks([0,1,2,3,4])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig('LSTM_v3_Precision.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, accuracy, AUC , Precision, Recall = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import helper_module\n",
    "name = f'W2V_LSTM_v3'\n",
    "helper_module.add_model_scores_to_results(file_path = 'Model_results.csv', \n",
    "                            model_name = name,\n",
    "                            datashift = f'test', with_sw = 0,\n",
    "                            ROC_AUC = AUC, accuracy = accuracy, \n",
    "                            f1 = f1, recall=Recall, cm = np.zeros(4,), first_entry=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir -p saved_model\n",
    "model.save('saved_model/lstm_v3')\n",
    "\n",
    "# new_model = tf.keras.models.load_model('saved_model/lstm_v2')\n",
    "# loss, accuracy, AUC , Precision, Recall = new_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary Data shift\n",
    "\n",
    "rating_df = pd.DataFrame(df, columns=['Score_class', 'cleaned_summary'])\n",
    "dummies = pd.get_dummies(rating_df['Score_class'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(rating_df['cleaned_summary'], \n",
    "                                                    dummies, \n",
    "                                                    test_size=0.1, random_state = 42\n",
    ")\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(x_test, maxlen=512)\n",
    "\n",
    "loss, accuracy, AUC , Precision, Recall = model.evaluate(x_test, y_test, verbose=2)\n",
    "f1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "name = f'W2V_LSTM_v3'\n",
    "helper_module.add_model_scores_to_results(file_path = 'Model_results.csv', \n",
    "                            model_name = name,\n",
    "                            datashift = f'summary', with_sw = 0,\n",
    "                            ROC_AUC = AUC, accuracy = accuracy, \n",
    "                            f1 = f1, recall=Recall, cm = np.zeros(4,), first_entry=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout 0.1\n",
    "\n",
    "rating_df = pd.DataFrame(df, columns=['Score_class', 'cleaned_text'])\n",
    "dummies = pd.get_dummies(rating_df['Score_class'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(rating_df['cleaned_text'], \n",
    "                                                    dummies, \n",
    "                                                    test_size=0.1, random_state = 42\n",
    ")\n",
    "x_test = x_test.apply(helper_module.random_dropout,\n",
    "                      p=0.1, \n",
    "                      random_state=42)\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(x_test, maxlen=512)\n",
    "\n",
    "loss, accuracy, AUC , Precision, Recall = model.evaluate(x_test, y_test, verbose=2)\n",
    "f1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "name = f'W2V_LSTM_v3'\n",
    "helper_module.add_model_scores_to_results(file_path = 'Model_results.csv', \n",
    "                            model_name = name,\n",
    "                            datashift = f'dropout_0.1', with_sw = 0,\n",
    "                            ROC_AUC = AUC, accuracy = accuracy, \n",
    "                            f1 = f1, recall=Recall, cm = np.zeros(4,), first_entry=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout 0.25\n",
    "\n",
    "rating_df = pd.DataFrame(df, columns=['Score_class', 'cleaned_text'])\n",
    "dummies = pd.get_dummies(rating_df['Score_class'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(rating_df['cleaned_text'], \n",
    "                                                    dummies, \n",
    "                                                    test_size=0.1, random_state = 42\n",
    ")\n",
    "x_test = x_test.apply(helper_module.random_dropout,\n",
    "                      p=0.25, \n",
    "                      random_state=42)\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(x_test, maxlen=512)\n",
    "\n",
    "loss, accuracy, AUC , Precision, Recall = model.evaluate(x_test, y_test, verbose=2)\n",
    "f1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "name = f'W2V_LSTM_v3'\n",
    "helper_module.add_model_scores_to_results(file_path = 'Model_results.csv', \n",
    "                            model_name = name,\n",
    "                            datashift = f'dropout_0.25', with_sw = 0,\n",
    "                            ROC_AUC = AUC, accuracy = accuracy, \n",
    "                            f1 = f1, recall=Recall, cm = np.zeros(4,), first_entry=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout 0.5\n",
    "\n",
    "rating_df = pd.DataFrame(df, columns=['Score_class', 'cleaned_text'])\n",
    "dummies = pd.get_dummies(rating_df['Score_class'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(rating_df['cleaned_text'], \n",
    "                                                    dummies, \n",
    "                                                    test_size=0.1, random_state = 42\n",
    ")\n",
    "x_test = x_test.apply(helper_module.random_dropout,\n",
    "                      p=0.5, \n",
    "                      random_state=42)\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(x_test, maxlen=512)\n",
    "\n",
    "loss, accuracy, AUC , Precision, Recall = model.evaluate(x_test, y_test, verbose=2)\n",
    "f1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "name = f'W2V_LSTM_v3'\n",
    "helper_module.add_model_scores_to_results(file_path = 'Model_results.csv', \n",
    "                            model_name = name,\n",
    "                            datashift = f'dropout_0.5', with_sw = 0,\n",
    "                            ROC_AUC = AUC, accuracy = accuracy, \n",
    "                            f1 = f1, recall=Recall, cm = np.zeros(4,), first_entry=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference: \n",
    "**Amazon Fine Food Reviews: Sentiment Analysis.**\n",
    "Provides the basic guidelines for LSTM. \n",
    "https://www.kaggle.com/code/chirag9073/amazon-fine-food-reviews-sentiment-analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
